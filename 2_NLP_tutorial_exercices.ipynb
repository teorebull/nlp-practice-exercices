{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20a7c4a8-262d-4cb6-a447-779eebb4f4f4",
   "metadata": {},
   "source": [
    "# Exercise for Spacy POS tutorial,\n",
    "\n",
    "You are parsing a news story from cnbc.com. News story is stores in news_story.txt which is available in this same folder on github. You need to,\n",
    "i. Extract all NOUN tokens from this story. You will have to read the file in python first to collect all the text and then extract NOUNs in a python list\n",
    "ii. Extract all numbers (NUM POS type) in a python list\n",
    "iii. Print a count of all POS tags in this story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "33c165ef-d390-4054-b237-e41e42d8f8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Inflation, 'NOUN'), (climb, 'NOUN'), (consumers, 'NOUN'), (brink, 'NOUN'), (expansion, 'NOUN'), (consumer, 'NOUN'), (price, 'NOUN'), (index, 'NOUN'), (measure, 'NOUN'), (prices, 'NOUN'), (goods, 'NOUN'), (services, 'NOUN'), (%, 'NOUN'), (year, 'NOUN'), (estimate, 'NOUN'), (%, 'NOUN'), (gain, 'NOUN'), (ease, 'NOUN'), (peak, 'NOUN'), (level, 'NOUN'), (summer, 'NOUN'), (food, 'NOUN'), (energy, 'NOUN'), (prices, 'NOUN'), (core, 'NOUN'), (%, 'NOUN'), (expectations, 'NOUN'), (%, 'NOUN'), (gain, 'NOUN'), (hopes, 'NOUN'), (inflation, 'NOUN'), (month, 'NOUN'), (month, 'NOUN'), (gains, 'NOUN'), (expectations, 'NOUN'), (%, 'NOUN'), (headline, 'NOUN'), (%, 'NOUN'), (estimate, 'NOUN'), (%, 'NOUN'), (increase, 'NOUN'), (core, 'NOUN'), (outlook, 'NOUN'), (%, 'NOUN'), (gain, 'NOUN'), (price, 'NOUN'), (gains, 'NOUN'), (workers, 'NOUN'), (ground, 'NOUN'), (wages, 'NOUN'), (inflation, 'NOUN'), (%, 'NOUN'), (month, 'NOUN'), (increase, 'NOUN'), (%, 'NOUN'), (earnings, 'NOUN'), (year, 'NOUN'), (earnings, 'NOUN'), (%, 'NOUN'), (earnings, 'NOUN'), (5.5%.Inflation, 'NOUN'), (threat, 'NOUN'), (recovery, 'NOUN'), (pandemic, 'NOUN'), (economy, 'NOUN'), (stage, 'NOUN'), (year, 'NOUN'), (growth, 'NOUN'), (level, 'NOUN'), (prices, 'NOUN'), (pump, 'NOUN'), (grocery, 'NOUN'), (stores, 'NOUN'), (problem, 'NOUN'), (inflation, 'NOUN'), (areas, 'NOUN'), (housing, 'NOUN'), (auto, 'NOUN'), (sales, 'NOUN'), (host, 'NOUN'), (areas, 'NOUN'), (officials, 'NOUN'), (problem, 'NOUN'), (interest, 'NOUN'), (rate, 'NOUN'), (hikes, 'NOUN'), (year, 'NOUN'), (pledges, 'NOUN'), (inflation, 'NOUN'), (%, 'NOUN'), (goal, 'NOUN'), (â„¢, 'NOUN'), (data, 'NOUN'), (job, 'NOUN'), (Credits, 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "token_list = []\n",
    "text = ''\n",
    "with open('news_story.txt') as file:\n",
    "    file = file.readlines()\n",
    "    for line in file:\n",
    "        text += line.strip()\n",
    "            \n",
    "# Tokenization and getting NOUNs\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        token_list.append((token, token.pos_))\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "755b9031-4f5e-4c60-93b1-8b1ca8466eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8.3, 'NUM'), (8.1, 'NUM'), (6.2, 'NUM'), (6, 'NUM'), (0.3, 'NUM'), (0.2, 'NUM'), (0.6, 'NUM'), (0.4, 'NUM'), (0.1, 'NUM'), (0.3, 'NUM'), (2.6, 'NUM'), (2021, 'NUM'), (1984, 'NUM'), (one, 'NUM'), (two, 'NUM'), (two, 'NUM'), (2, 'NUM')]\n"
     ]
    }
   ],
   "source": [
    "# ii. Extract all numbers (NUM POS type) in a python list\n",
    "\n",
    "numbers = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == 'NUM':\n",
    "        numbers.append((token, token.pos_))\n",
    "\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7b820058-9925-4a20-b5cd-cb65d724d6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{92: 95,\n",
       " 100: 27,\n",
       " 86: 15,\n",
       " 85: 39,\n",
       " 96: 20,\n",
       " 97: 30,\n",
       " 90: 34,\n",
       " 95: 4,\n",
       " 87: 13,\n",
       " 89: 10,\n",
       " 84: 23,\n",
       " 93: 17,\n",
       " 94: 4,\n",
       " 98: 8,\n",
       " 101: 1}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iii. Print a count of all POS tags in this story\n",
    "\n",
    "count = doc.count_by(spacy.attrs.POS)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e2a71e1a-fed7-4eb4-852b-1681eb849eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN | 95\n",
      "VERB | 27\n",
      "ADV | 15\n",
      "ADP | 39\n",
      "PROPN | 20\n",
      "PUNCT | 30\n",
      "DET | 34\n",
      "PRON | 4\n",
      "AUX | 13\n",
      "CCONJ | 10\n",
      "ADJ | 23\n",
      "NUM | 17\n",
      "PART | 4\n",
      "SCONJ | 8\n",
      "X | 1\n"
     ]
    }
   ],
   "source": [
    "for k,v in count.items():\n",
    "\n",
    "    print(doc.vocab[k].text, \"|\",v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7544f0bd-cac6-4521-bb01-582149353f94",
   "metadata": {},
   "source": [
    "# Displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2b5eb69d-b161-4d48-82c1-b580a9e5fa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "McDonald's | ORG | Companies, agencies, institutions, etc.\n",
      "Tesla | ORG | Companies, agencies, institutions, etc.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Hello Mate, I love burgers from \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    McDonald's\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", that's fasho dude. Elon Musk be earning those G's from \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy import displacy \n",
    "\n",
    "text = \"Hello Mate, I love burgers from McDonald's, that's fasho dude. Elon Musk be earning those G's from Tesla\"\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent, \"|\", ent.label_, '|', spacy.explain(ent.label_))\n",
    "    \n",
    "\n",
    "displacy.render(doc, style='ent')\n",
    "\n",
    "nlp.pipe_labels['ner']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d79f63-ed75-490f-8e0d-3bc1862b4f4a",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER): Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e452465-5216-499a-94a6-6fe63267ee2c",
   "metadata": {},
   "source": [
    "Excercice: 1\n",
    "Extract all the Geographical (cities, Countries, states) names from a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "be332533-1495-45d4-a741-0a412499685b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']\n",
      "[Kiran, India, Delhi, Gujarat, Tamilnadu, Andhrapradesh, Assam, Bihar]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Kiran want to know the famous foods in each state of India. So, he opened Google and search for this question. Google showed that\n",
    "in Delhi it is Chaat, in Gujarat it is Dal Dhokli, in Tamilnadu it is Pongal, in Andhrapradesh it is Biryani, in Assam it is Papaya Khar,\n",
    "in Bihar it is Litti Chowkha and so on for all other states\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(nlp.pipe_labels['ner'])\n",
    "\n",
    "entities = []\n",
    "\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'GPE':\n",
    "        entities.append(ent)\n",
    "print(entities)\n",
    "print(len(entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336cfb62-c9f4-41a5-b01f-097d900c727e",
   "metadata": {},
   "source": [
    "Excersie: 2\n",
    "Extract all the birth dates of cricketers in the given Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ba23a24e-af7b-4f4c-80c4-1bac21e96cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24 April 1973, 5 November 1988, 7 July 1981, 19 December 1974]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Sachin Tendulkar was born on 24 April 1973, Virat Kholi was born on 5 November 1988, Dhoni was born on 7 July 1981\n",
    "and finally Ricky ponting was born on 19 December 1974.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "entities = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'DATE':\n",
    "        entities.append(ent)\n",
    "        \n",
    "print(entities)\n",
    "print(len(entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94feca0-61a2-43c7-bcf6-5e6972f02fd0",
   "metadata": {},
   "source": [
    "# Bag of words: Exercises\n",
    "In this Exercise, you are going to classify whether a given movie review is positive or negative.\n",
    "you are going to use Bag of words for pre-processing the text and apply different classification algorithms.\n",
    "Sklearn CountVectorizer has the inbuilt implementations for Bag of Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a4fdc196-3410-4da0-b63a-6b2bf1df9d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from  sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1e226bc4-c6d7-444a-a362-fe4d6e9b9755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "#1. read the data provided in the same directory with name 'movies_sentiment_data.csv' and store it in df variable\n",
    "df = pd.read_csv('IMDB Dataset.csv').head(10000)\n",
    "\n",
    "\n",
    "#2. print the shape of the data\n",
    "print(df.shape)\n",
    "\n",
    "#3. print top 5 datapoints\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c73d7ec5-0cb1-4192-850c-7c4fbf8572e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "1    5028\n",
      "0    4972\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8000,)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a new column \"Category\" which represent 1 if the sentiment is positive or 0 if it is negative\n",
    "df['category'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "data = df['review']\n",
    "target = df['category']\n",
    "#check the distribution of 'Category' and see whether the Target labels are balanced or not.\n",
    "print(df['category'].value_counts())\n",
    "\n",
    "#Do the 'train-test' splitting with test size of 20%\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, target, test_size = 0.2)\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f151ad-dafd-49b6-95d4-06c6e5a36834",
   "metadata": {},
   "source": [
    "#### Exercise-1\n",
    "\n",
    "using sklearn pipeline module create a classification pipeline to classify the movie review's positive or negative.\n",
    "Note:\n",
    "\n",
    "use CountVectorizer for pre-processing the text.\n",
    "\n",
    "use Random Forest as the classifier with estimators as 50 and criterion as entropy.\n",
    "\n",
    "print the classification report.\n",
    "\n",
    "References:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1ef9cc7e-b3f1-4c81-8ce3-41f3354635c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.82      0.81       984\n",
      "           1       0.82      0.81      0.81      1016\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.81      0.81      0.81      2000\n",
      "weighted avg       0.81      0.81      0.81      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. create a pipeline object\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),                                                    #initializing the vectorizer\n",
    "    ('random_forest', (RandomForestClassifier(n_estimators=50, criterion='entropy')))      #using the RandomForest classifier\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "score = classification_report(y_test, y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c24a5b-d8dd-4af3-9bd9-8ec8ccdff92c",
   "metadata": {},
   "source": [
    "#### Exercise-2\n",
    "\n",
    "using sklearn pipeline module create a classification pipeline to classify the movie review's positive or negative..\n",
    "Note:\n",
    "\n",
    "use CountVectorizer for pre-processing the text.\n",
    "use KNN as the classifier with n_neighbors of 10 and metric as 'euclidean'.\n",
    "print the classification report.\n",
    "References:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0de0d872-d5f8-4e73-a613-5550c0a80e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.68      0.65       984\n",
      "           1       0.66      0.61      0.63      1016\n",
      "\n",
      "    accuracy                           0.64      2000\n",
      "   macro avg       0.65      0.65      0.64      2000\n",
      "weighted avg       0.65      0.64      0.64      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. create a pipeline object\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),                                                    #initializing the vectorizer\n",
    "    ('knn', (KNeighborsClassifier(n_neighbors=10, metric='euclidean')))      #using the KNN classifier\n",
    "])\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "#4. print the classfication report\n",
    "score = classification_report(y_test, y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0fdf07-8576-4235-baec-4e81abf4e37a",
   "metadata": {},
   "source": [
    "#### Exercise-3\n",
    "\n",
    "using sklearn pipeline module create a classification pipeline to classify the movie review's positive or negative..\n",
    "Note:\n",
    "\n",
    "use CountVectorizer for pre-processing the text.\n",
    "use Multinomial Naive Bayes as the classifier.\n",
    "print the classification report.\n",
    "References:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0220aba7-3cea-4d24-bca1-cd34e0ae07c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84       984\n",
      "           1       0.87      0.80      0.83      1016\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.84      0.84      0.84      2000\n",
      "weighted avg       0.84      0.84      0.84      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. create a pipeline object\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),                                                    #initializing the vectorizer\n",
    "    ('naive_bayes', (MultinomialNB()))      #using the NB classifier\n",
    "])\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "#4. print the classfication report\n",
    "score = classification_report(y_test, y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff0f2bb-5f6b-4728-84a2-2ab3c887479d",
   "metadata": {},
   "source": [
    "# Stop Words: Exercise\n",
    "Run this cell to import all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1cb7ef54-29ee-4bc4-b5e4-d188a6791e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import spacy and load the model\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a7b03b-f33a-4dce-9d7d-335b696970b3",
   "metadata": {},
   "source": [
    "#### Exercise1:\n",
    "\n",
    "From a Given Text, Count the number of stop words in it.\n",
    "Print the percentage of stop word tokens compared to all tokens in a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f7b4f7b0-07ee-4ecc-9c59-648d6002008d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "Percentage of stop words: 25.0 %\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "Thor: Love and Thunder is a 2022 American superhero film based on Marvel Comics featuring the character Thor, produced by Marvel Studios and \n",
    "distributed by Walt Disney Studios Motion Pictures. It is the sequel to Thor: Ragnarok (2017) and the 29th film in the Marvel Cinematic Universe (MCU).\n",
    "The film is directed by Taika Waititi, who co-wrote the script with Jennifer Kaytin Robinson, and stars Chris Hemsworth as Thor alongside Christian Bale, Tessa Thompson,\n",
    "Jaimie Alexander, Waititi, Russell Crowe, and Natalie Portman. In the film, Thor attempts to find inner peace, but must return to action and recruit Valkyrie (Thompson),\n",
    "Korg (Waititi), and Jane Foster (Portman)â€”who is now the Mighty Thorâ€”to stop Gorr the God Butcher (Bale) from eliminating all gods.\n",
    "'''\n",
    "\n",
    "#step1: Create the object 'doc' for the given text using nlp()\n",
    "doc = nlp(text)\n",
    "\n",
    "\n",
    "#step2: define the variables to keep track of stopwords count and total words count\n",
    "from spacy.lang.en import stop_words\n",
    "\n",
    "stop_words = stop_words.STOP_WORDS\n",
    "\n",
    "\n",
    "stop_words_count = 0\n",
    "words_count = 0\n",
    "\n",
    "#step3: iterate through all the words in the document\n",
    "for token in doc:\n",
    "    if token.text.lower() in stop_words:\n",
    "        stop_words_count += 1\n",
    "    else:\n",
    "        words_count += 1\n",
    "    \n",
    "#step4: print the count of stop words\n",
    "print(stop_words_count)\n",
    "\n",
    "#step5: print the percentage of stop words compared to total words in the text\n",
    "percentage = stop_words_count / (stop_words_count + words_count)\n",
    "print('Percentage of stop words:', percentage * 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5ee7e5-b334-4f03-b4d7-0aa4a4e92adb",
   "metadata": {},
   "source": [
    "#### Exercise2:\n",
    "\n",
    "Spacy default implementation considers \"not\" as a stop word. But in some scenarios removing 'not' will completely change the meaning of the statement/text. For Example, consider these two statements:\n",
    "\n",
    "- this is a good movie       ----> Positive Statement\n",
    "- this is not a good movie   ----> Negative Statement\n",
    "So, after applying stopwords to those 2 texts, both will return \"good movie\" and does not respect the polarity/sentiments of text.\n",
    "\n",
    "Now, your task is to remove this stop word \"not\" in spaCy and help in distinguishing the texts.\n",
    "\n",
    "Hint: GOOGLE IT! Google is your friend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a4f54ca7-8a85-439c-8b92-e64bfb0d76c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: good movie\n",
      "Text 2: not good movie\n"
     ]
    }
   ],
   "source": [
    "#use this pre-processing function to pass the text and to remove all the stop words and finally get the cleaned form\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    no_stop_words = [token.text for token in doc if not token.is_stop]\n",
    "    return \" \".join(no_stop_words)       \n",
    "\n",
    "#Step1: remove the stopword 'not' in spacy\n",
    "nlp.vocab['not'].is_stop = False\n",
    "\n",
    "#step2: send the two texts given above into the pre-process function and store the transformed texts\n",
    "text1 = preprocess('this is a good movie')\n",
    "text2 = preprocess('this is not a good movie')\n",
    "\n",
    "\n",
    "#step3: finally print those 2 transformed texts\n",
    "print('Text 1:', text1)\n",
    "print('Text 2:', text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd8d6c8-7dbf-4499-a84b-96d8892a3a1e",
   "metadata": {},
   "source": [
    "#### Exercise3:\n",
    "\n",
    "From a given text, output the most frequently used token after removing all the stop word tokens and punctuations in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "d78092fc-7c60-418c-aa3b-752af638cb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum frequency word: India\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "text = ''' The India men's national cricket team, also known as Team India or the Men in Blue, represents India in men's international cricket.\n",
    "It is governed by the Board of Control for Cricket in India (BCCI), and is a Full Member of the International Cricket Council (ICC) with Test,\n",
    "One Day International (ODI) and Twenty20 International (T20I) status. Cricket was introduced to India by British sailors in the 18th century, and the \n",
    "first cricket club was established in 1792. India's national cricket team played its first Test match on 25 June 1932 at Lord's, becoming the sixth team to be\n",
    "granted test cricket status.\n",
    "'''\n",
    "\n",
    "\n",
    "#step1: Create the object 'doc' for the given text using nlp()\n",
    "doc = nlp(text)\n",
    "\n",
    "\n",
    "#step2: remove all the stop words and punctuations and store all the remaining tokens in a new list\n",
    "remaining_tokens = []\n",
    "for token in doc:\n",
    "  if token.is_stop or token.is_punct:    #check whether a given token is stop word or punctuations\n",
    "    continue\n",
    "  remaining_tokens.append(token.text)\n",
    "\n",
    "\n",
    "#step3: create a new dictionary and get the frequency of words by iterating through the list which contains stored tokens  \n",
    "frequency_tokens = {}\n",
    "for token in remaining_tokens:\n",
    "  if token != '\\n' and token != ' ':      #As spacy considers new line and empty spaces as seperate token, it's better to ignore them\n",
    "    if token not in frequency_tokens:     #if a particular token occurs for the first time, we initialise it to 1\n",
    "      frequency_tokens[token] = 1\n",
    "    else:\n",
    "      frequency_tokens[token] += 1        #if a partcular token is already present, then increment by 1 based on value already presented\n",
    "\n",
    "\n",
    "#step4: get the maximum frequency word\n",
    "max_freq_word = max(frequency_tokens.keys(), key=(lambda key: frequency_tokens[key]))\n",
    "\n",
    "\n",
    "#step5: finally print the result\n",
    "print(f\"Maximum frequency word: {max_freq_word}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf2fd7-4cf9-4d44-8df8-5fa22b9f8a6b",
   "metadata": {},
   "source": [
    "# Bag of n_grams: Exercise\n",
    "\n",
    "Fake news refers to misinformation or disinformation in the country which is spread through word of mouth and more recently through digital communication such as What's app messages, social media posts, etc.\n",
    "\n",
    "Fake news spreads faster than Real news and creates problems and fear among groups and in society.\n",
    "\n",
    "We are going to address these problems using classical NLP techniques and going to classify whether a given message/ text is Real or Fake Message.\n",
    "\n",
    "You will use a Bag of n-grams to pre-process the text and apply different classification algorithms.\n",
    "\n",
    "Sklearn CountVectorizer has the inbuilt implementations for Bag of Words.\n",
    "\n",
    "About Data: Fake News Detection\n",
    "Credits: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\n",
    "\n",
    "This data consists of two columns. - Text - label\n",
    "\n",
    "Text is the statements or messages regarding a particular event/situation.\n",
    "\n",
    "label feature tells whether the given Text is Fake or Real.\n",
    "\n",
    "As there are only 2 classes, this problem comes under the Binary Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71101e62-fc73-4a9f-bd8f-0b1164b8ecf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Yearâ€™...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obamaâ€™s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21412</th>\n",
       "      <td>'Fully committed' NATO backs new U.S. approach...</td>\n",
       "      <td>BRUSSELS (Reuters) - NATO allies on Tuesday we...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21413</th>\n",
       "      <td>LexisNexis withdrew two products from Chinese ...</td>\n",
       "      <td>LONDON (Reuters) - LexisNexis, a provider of l...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21414</th>\n",
       "      <td>Minsk cultural hub becomes haven from authorities</td>\n",
       "      <td>MINSK (Reuters) - In the shadow of disused Sov...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21415</th>\n",
       "      <td>Vatican upbeat on possibility of Pope Francis ...</td>\n",
       "      <td>MOSCOW (Reuters) - Vatican Secretary of State ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21416</th>\n",
       "      <td>Indonesia to buy $1.14 billion worth of Russia...</td>\n",
       "      <td>JAKARTA (Reuters) - Indonesia will buy 11 Sukh...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0       Donald Trump Sends Out Embarrassing New Yearâ€™...   \n",
       "1       Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2       Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3       Trump Is So Obsessed He Even Has Obamaâ€™s Name...   \n",
       "4       Pope Francis Just Called Out Donald Trump Dur...   \n",
       "...                                                  ...   \n",
       "21412  'Fully committed' NATO backs new U.S. approach...   \n",
       "21413  LexisNexis withdrew two products from Chinese ...   \n",
       "21414  Minsk cultural hub becomes haven from authorities   \n",
       "21415  Vatican upbeat on possibility of Pope Francis ...   \n",
       "21416  Indonesia to buy $1.14 billion worth of Russia...   \n",
       "\n",
       "                                                    text    subject  \\\n",
       "0      Donald Trump just couldn t wish all Americans ...       News   \n",
       "1      House Intelligence Committee Chairman Devin Nu...       News   \n",
       "2      On Friday, it was revealed that former Milwauk...       News   \n",
       "3      On Christmas day, Donald Trump announced that ...       News   \n",
       "4      Pope Francis used his annual Christmas Day mes...       News   \n",
       "...                                                  ...        ...   \n",
       "21412  BRUSSELS (Reuters) - NATO allies on Tuesday we...  worldnews   \n",
       "21413  LONDON (Reuters) - LexisNexis, a provider of l...  worldnews   \n",
       "21414  MINSK (Reuters) - In the shadow of disused Sov...  worldnews   \n",
       "21415  MOSCOW (Reuters) - Vatican Secretary of State ...  worldnews   \n",
       "21416  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...  worldnews   \n",
       "\n",
       "                    date category  \n",
       "0      December 31, 2017     Fake  \n",
       "1      December 31, 2017     Fake  \n",
       "2      December 30, 2017     Fake  \n",
       "3      December 29, 2017     Fake  \n",
       "4      December 25, 2017     Fake  \n",
       "...                  ...      ...  \n",
       "21412   August 22, 2017      True  \n",
       "21413   August 22, 2017      True  \n",
       "21414   August 22, 2017      True  \n",
       "21415   August 22, 2017      True  \n",
       "21416   August 22, 2017      True  \n",
       "\n",
       "[44898 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_fake = pd.read_csv(r\"C:\\Users\\teore\\OneDrive\\Documents\\GitHub\\NLP_tutorial\\Fake.csv\")\n",
    "df_true = pd.read_csv(r\"C:\\Users\\teore\\OneDrive\\Documents\\GitHub\\NLP_tutorial\\True.csv\")\n",
    "\n",
    "df_fake['category'] = 'Fake'\n",
    "df_true['category'] = 'True'\n",
    "\n",
    "df_combined = pd.concat([df_fake, df_true])\n",
    "df_combined.to_csv(r\"C:\\Users\\teore\\OneDrive\\Documents\\GitHub\\NLP_tutorial\\Fake_Real_Data.csv\")\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f25557d-b5d1-4a43-ad74-7dec4a4f90bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44898, 6)\n",
      "   Unnamed: 0                                              title  \\\n",
      "0           0   Donald Trump Sends Out Embarrassing New Yearâ€™...   \n",
      "1           1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "2           2   Sheriff David Clarke Becomes An Internet Joke...   \n",
      "3           3   Trump Is So Obsessed He Even Has Obamaâ€™s Name...   \n",
      "4           4   Pope Francis Just Called Out Donald Trump Dur...   \n",
      "\n",
      "                                                text subject  \\\n",
      "0  Donald Trump just couldn t wish all Americans ...    News   \n",
      "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
      "2  On Friday, it was revealed that former Milwauk...    News   \n",
      "3  On Christmas day, Donald Trump announced that ...    News   \n",
      "4  Pope Francis used his annual Christmas Day mes...    News   \n",
      "\n",
      "                date category  \n",
      "0  December 31, 2017     Fake  \n",
      "1  December 31, 2017     Fake  \n",
      "2  December 30, 2017     Fake  \n",
      "3  December 29, 2017     Fake  \n",
      "4  December 25, 2017     Fake  \n",
      "subject\n",
      "politicsNews       11272\n",
      "worldnews          10145\n",
      "News                9050\n",
      "politics            6841\n",
      "left-news           4459\n",
      "Government News     1570\n",
      "US_News              783\n",
      "Middle-east          778\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Yearâ€™...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Trump Is So Obsessed He Even Has Obamaâ€™s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0   Donald Trump Sends Out Embarrassing New Yearâ€™...   \n",
       "1           1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2           2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3           3   Trump Is So Obsessed He Even Has Obamaâ€™s Name...   \n",
       "4           4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date category  label_num  \n",
       "0  December 31, 2017     Fake          0  \n",
       "1  December 31, 2017     Fake          0  \n",
       "2  December 30, 2017     Fake          0  \n",
       "3  December 29, 2017     Fake          0  \n",
       "4  December 25, 2017     Fake          0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "#read the dataset with name \"Fake_Real_Data.csv\" and store it in a variable df\n",
    "df = pd.read_csv(r\"C:\\Users\\teore\\OneDrive\\Documents\\GitHub\\NLP_tutorial\\Fake_Real_Data.csv\")\n",
    "\n",
    "#print the shape of dataframe\n",
    "print(df.shape)\n",
    "\n",
    "#print top 5 rows\n",
    "print(df.head())\n",
    "\n",
    "#check the distribution of labels \n",
    "print(df['subject'].value_counts())\n",
    "\n",
    "#Add the new column \"label_num\" which gives a unique number to each of these labels \n",
    "df['label_num'] = df['category'].map({'Fake': 0, 'True': 1})\n",
    "\n",
    "#check the results with top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9acb56e-f83e-44d1-8397-077040598767",
   "metadata": {},
   "source": [
    "# Modelling without Pre-processing Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f089ae7-9017-4ce6-a94d-b98ddf90f508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size (8000,)\n",
      "test_size (2000,)\n",
      "\n",
      "\n",
      "label_num\n",
      "0    5254\n",
      "1    4746\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Reduce sample\n",
    "df_sampled = df.sample(n=10_000, random_state=42)\n",
    "\n",
    "# import train-test-split from sklearn \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "\n",
    "#Do the 'train-test' splitting with test size of 20% with random state of 2022 and stratify sampling too\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_sampled['text'], \n",
    "    df_sampled['label_num'], \n",
    "    test_size=0.2, \n",
    "    random_state=2022,\n",
    "    stratify=df_sampled['label_num']\n",
    ")\n",
    "\n",
    "#print the shapes of X_train and X_test\n",
    "print('train_size', X_train.shape)\n",
    "print('test_size', X_test.shape)\n",
    "print('\\n')\n",
    "print(df_sampled['label_num'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e8bcdf-18c2-4758-a830-eb0abef7b45d",
   "metadata": {},
   "source": [
    "### Attempt 1 :\n",
    "\n",
    "using sklearn pipeline module create a classification pipeline to classify the Data.\n",
    "Note:\n",
    "\n",
    "using CountVectorizer with unigram, bigram, and trigrams.\n",
    "use KNN as the classifier with n_neighbors of 10 and metric as 'euclidean' distance.\n",
    "print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2888fd8f-b83f-40fc-8d5b-407064160639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.75      1051\n",
      "           1       0.73      0.69      0.71       949\n",
      "\n",
      "    accuracy                           0.73      2000\n",
      "   macro avg       0.73      0.73      0.73      2000\n",
      "weighted avg       0.73      0.73      0.73      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#1. create a pipeline object\n",
    "knn = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range = (1,3))),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=10, metric='euclidean'))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072741be-fc4a-4e31-82f2-0dc486be9a1c",
   "metadata": {},
   "source": [
    "### Attempt 2 :\n",
    "\n",
    "using the sklearn pipeline module create a classification pipeline to classify the Data.\n",
    "Note:\n",
    "\n",
    "using CountVectorizer with unigram, bigram, and trigrams.\n",
    "use KNN as the classifier with n_neighbors of 10 and metric as 'cosine' distance.\n",
    "print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "236c873f-758e-44a0-ac62-5aac78089493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.98      0.75      1051\n",
      "           1       0.93      0.30      0.46       949\n",
      "\n",
      "    accuracy                           0.66      2000\n",
      "   macro avg       0.77      0.64      0.60      2000\n",
      "weighted avg       0.76      0.66      0.61      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#1. create a pipeline object\n",
    "knn = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range = (1,3))),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=10, metric='cosine'))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5baa2a-92bf-42c0-9354-2428caf05bd3",
   "metadata": {},
   "source": [
    "### Attempt 3 :\n",
    "\n",
    "using the sklearn pipeline module create a classification pipeline to classify the Data.\n",
    "Note:\n",
    "\n",
    "using CountVectorizer with only trigrams.\n",
    "use RandomForest as the classifier.\n",
    "print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c041356-203f-4ce8-b347-2ddee914cb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94      1051\n",
      "           1       0.95      0.91      0.93       949\n",
      "\n",
      "    accuracy                           0.94      2000\n",
      "   macro avg       0.94      0.93      0.94      2000\n",
      "weighted avg       0.94      0.94      0.94      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#1. create a pipeline object\n",
    "rf = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range = (3,3))),\n",
    "    ('random_forest', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343c29c-4eed-4042-8425-66807d45fe90",
   "metadata": {},
   "source": [
    "### Attempt 4 :\n",
    "\n",
    "using the sklearn pipeline module create a classification pipeline to classify the Data.\n",
    "Note:\n",
    "\n",
    "using CountVectorizer with both unigram and bigrams.\n",
    "use Multinomial Naive Bayes as the classifier with an alpha value of 0.75.\n",
    "print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70884103-eeb3-45cc-bb3c-c2d9e79cf77c",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 15\u001b[0m\n\u001b[0;32m      7\u001b[0m nb \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m      8\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorizer\u001b[39m\u001b[38;5;124m'\u001b[39m, CountVectorizer(ngram_range \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))),\n\u001b[0;32m      9\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnb\u001b[39m\u001b[38;5;124m'\u001b[39m, MultinomialNB(alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.75\u001b[39m))\n\u001b[0;32m     10\u001b[0m ])\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#2. fit with X_train and y_train\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#3. get the predictions for X_test and store it in y_pred\u001b[39;00m\n\u001b[0;32m     19\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m nb\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Lib\\site-packages\\sklearn\\pipeline.py:469\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    468\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 469\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\Lib\\site-packages\\sklearn\\pipeline.py:406\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params)\u001b[0m\n\u001b[0;32m    404\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    405\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32m~\\Lib\\site-packages\\joblib\\memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Lib\\site-packages\\sklearn\\pipeline.py:1310\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1310\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1312\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1313\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1314\u001b[0m         )\n",
      "File \u001b[1;32m~\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1266\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m   1267\u001b[0m         \u001b[38;5;66;03m# Ignore out-of-vocabulary items for fixed_vocab=True\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1270\u001b[0m \u001b[43mj_indices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_counter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1271\u001b[0m values\u001b[38;5;241m.\u001b[39mextend(feature_counter\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m   1272\u001b[0m indptr\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(j_indices))\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#1. create a pipeline object\n",
    "nb = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range = (1,2))),\n",
    "    ('nb', MultinomialNB(alpha = 0.75))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a62ef0-6184-462a-be90-adbb6e45b7d2",
   "metadata": {},
   "source": [
    "### Use text pre-processing to remove stop words, punctuations and apply lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2adbd7-6dd3-4fb1-9817-f3be01f00176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this utility function to get the preprocessed text data\n",
    "import spacy\n",
    "\n",
    "# load english language model and create nlp object from it\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "def preprocess(text):\n",
    "    # remove stop words and lemmatize the text\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "    \n",
    "    return \" \".join(filtered_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f7c2a0-77a1-4583-b3de-00e0a6805b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column \"preprocessed_txt\" and use the utility function above to get the clean data\n",
    "\n",
    "# Separate the data into two classes\n",
    "df_true = df[df['label_num'] == 1].sample(n=1, random_state=42)\n",
    "df_fake = df[df['label_num'] == 0].sample(n=1, random_state=42)\n",
    "\n",
    "# Combine them to create a balanced dataset\n",
    "df_balanced = pd.concat([df_true, df_fake]).sample(frac=1, random_state=42)  # Shuffle the dataset\n",
    "\n",
    "# Print the new class distribution\n",
    "print(df_balanced['label_num'].value_counts())\n",
    "\n",
    "df_balanced[\"preprocessed_txt\"] = df['text'].apply(preprocess)\n",
    "# this will take some time, please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e8190-5a8f-46d0-ac1b-b40854142344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the top 5 rows\n",
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8464fd-7f7d-41f0-a8fa-e4e82366d97c",
   "metadata": {},
   "source": [
    "### Build a model with pre processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4644a38-c0df-4c04-bf45-b4ecae5ab23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the 'train-test' splitting with test size of 20% with random state of 2022 and stratify sampling too\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['preprocessed_txt'], \n",
    "    df['label_num'], \n",
    "    test_size=0.2, \n",
    "    random_state=2022,\n",
    "    stratify=df['label_num']\n",
    ")\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Define the undersampler\n",
    "undersampler = RandomUnderSampler(sampling_strategy=1.0, random_state=42)\n",
    "\n",
    "# Resample both X_train and y_train\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train.to_frame(), y_train)\n",
    "\n",
    "# Convert back to Series\n",
    "X_train = X_train_resampled.squeeze()  # Ensure it's a Series\n",
    "y_train = y_train_resampled\n",
    "#Note: Make sure to use only the \"preprocessed_txt\" column for splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a0e685-be46-47e3-b954-0602bb22d93c",
   "metadata": {},
   "source": [
    "Let's check the scores with our best model till now\n",
    "\n",
    "### Random Forest\n",
    "### Attempt1 :\n",
    "\n",
    "using the sklearn pipeline module create a classification pipeline to classify the Data.\n",
    "Note:\n",
    "\n",
    "using CountVectorizer with only trigrams.\n",
    "use RandomForest as the classifier.\n",
    "print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c5faf-6d5a-4c20-8c54-739a7a9d88db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. create a pipeline object\n",
    "rf = Pipeline([('vectorizer', CountVectorizer(ngram_range = (3,3)), \n",
    "                ('random_forest', RandomForestClassifier()))])\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classfication_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0702e36e-8cfd-4d82-8f70-315bf44d217f",
   "metadata": {},
   "source": [
    "### Attempt2 :\n",
    "\n",
    "using the sklearn pipeline module create a classification pipeline to classify the Data.\n",
    "Note:\n",
    "\n",
    "using CountVectorizer with unigram, Bigram, and trigrams.\n",
    "use RandomForest as the classifier.\n",
    "print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340782ff-4bc6-4cdd-9ed5-9e178348d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. create a pipeline object\n",
    "rf = Pipeline([('vectorizer', CountVectorizer(ngram_range = (2,3)), \n",
    "                ('random_forest', RandomForestClassifier()))])\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classfication_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ad47fa-38da-4fb5-a58e-7edc63562c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally print the confusion matrix for the best model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e18fa-ac28-49ee-8f7c-024d718e2577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
